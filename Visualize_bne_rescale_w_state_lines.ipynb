{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm as norm_dist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed\n",
    "\n",
    "#sys.path.extend([os.getcwd()])\n",
    "\n",
    "from calibre.model import gaussian_process as gp\n",
    "from calibre.model import tailfree_process as tail_free\n",
    "from calibre.model import gp_regression_monotone as gpr_mono\n",
    "from calibre.model import adaptive_ensemble\n",
    "\n",
    "from calibre.inference import mcmc\n",
    "\n",
    "from calibre.calibration import score\n",
    "\n",
    "import calibre.util.misc as misc_util\n",
    "import calibre.util.metric as metric_util\n",
    "import calibre.util.visual as visual_util\n",
    "import calibre.util.matrix as matrix_util\n",
    "import calibre.util.ensemble as ensemble_util\n",
    "import calibre.util.calibration as calib_util\n",
    "\n",
    "import calibre.util.experiment_pred as pred_util\n",
    "\n",
    "from calibre.util.inference import make_value_setter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shapefile as shp\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_DICTIONARY = {\"root\": [\"AV\", \"GS\", \"CM\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = pd.read_csv(\"CA_2010_2016/CA_clean_training_2010_2016.csv\")\n",
    "\n",
    "X_train = np.asarray(y_obs[[\"lon\", \"lat\", \"time\"]].values.tolist()).astype(np.float32)\n",
    "\n",
    "base_train_feat = dict()\n",
    "\n",
    "for model_name in tail_free.get_leaf_model_names(_MODEL_DICTIONARY):\n",
    "    base_train_feat[model_name] = X_train\n",
    "   \n",
    "\n",
    "\"\"\" 1. prepare prediction data dictionary \"\"\"\n",
    "base_valid_feat = dict()\n",
    "\n",
    "\n",
    "for model_name in tail_free.get_leaf_model_names(_MODEL_DICTIONARY):\n",
    "    data_pd = pd.read_csv(\"CA_2010_2016/{}_2010_2016_align.csv\".format('CA_'+ model_name))\n",
    "    base_valid_feat[model_name] = np.asarray(data_pd[[\"lon\", \"lat\",\"time\"]].values.tolist()).astype(np.float32)\n",
    " \n",
    "X_valid = base_valid_feat[model_name]\n",
    "\n",
    "\"\"\" 3. standardize data \"\"\"\n",
    "# standardize\n",
    "X_centr = np.mean(X_valid, axis=0)\n",
    "X_scale = np.max(X_valid, axis=0) - np.min(X_valid, axis=0)\n",
    "\n",
    "X_scale_dist = np.max(X_scale[0:1])\n",
    "X_scale[0] = X_scale_dist\n",
    "X_scale[1] = X_scale_dist\n",
    "\n",
    "#X_valid = (X_valid - X_centr) / X_scale\n",
    "# X_train = (X_train - X_centr) / X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load from pickle file\n",
    "\n",
    "def load_pk(file_name):\n",
    "  data = []\n",
    "\n",
    "  for i in range(84):\n",
    "\n",
    "    with open('CA_2010_2016/hmc'+str(i)+'/'+file_name, 'rb') as f:\n",
    "        try:\n",
    "            while True:\n",
    "                data.append(pk.load(f))\n",
    "        except EOFError:\n",
    "            pass\n",
    "  return data \n",
    "  \n",
    "def get_overlapping_average(a):\n",
    "    df = pd.DataFrame(a)\n",
    "    out = df.groupby([0, 1, 2]).mean().reset_index()\n",
    "    return out.values\n",
    "\n",
    "ensemble_sample_val_mean = get_overlapping_average(np.concatenate(load_pk('ensemble_sample_val_mean.pkl'), axis=0))\n",
    "\n",
    "#print(ensemble_sample_val_mean.shape) \n",
    "#(723716, 4)\n",
    "#the 3rd column should be time\n",
    "#print(ensemble_sample_val_mean)\n",
    "\n",
    "\n",
    "ensemble_mean_val_mean = get_overlapping_average(np.concatenate(load_pk('ensemble_mean_val_mean.pkl'), axis=0))\n",
    "mean_resid = get_overlapping_average(np.concatenate(load_pk('mean_resid.pkl'), axis=0))\n",
    "\n",
    "ensemble_sample_val_var = get_overlapping_average(np.concatenate(load_pk('ensemble_sample_val_var.pkl'), axis=0))\n",
    "ensemble_mean_val_var = get_overlapping_average(np.concatenate(load_pk('ensemble_mean_val_var.pkl'), axis=0))\n",
    "uncn_resid = get_overlapping_average(np.concatenate(load_pk('uncn_resid.pkl'), axis=0))\n",
    "uncn_noise = get_overlapping_average(np.concatenate(load_pk('uncn_noise.pkl'), axis=0))\n",
    "\n",
    "ensemble_weights_val = get_overlapping_average(np.concatenate(load_pk('ensemble_weights_val.pkl'), axis=0))\n",
    "\n",
    "\n",
    "def filter_year(df, year):\n",
    "    return df[np.where(df[:, 2]==year)]\n",
    "\n",
    "#get unique years \n",
    "years = set(ensemble_sample_val_mean[:, 2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = shp.Reader('tl_2017_us_state/tl_2017_us_state.shp')\n",
    "sf_df = pd.DataFrame(sf.records())\n",
    "state_sf = np.take(sf.shapeRecords(), sf_df[sf_df[5].isin(['CA', 'NV', 'AZ'])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_posterior_heatmap_2d(plot_data, X,\n",
    "                         X_monitor=None,\n",
    "                         cmap='inferno_r',\n",
    "                         norm=None, norm_method=\"percentile\",\n",
    "                         save_addr=''):\n",
    "    \n",
    "    if save_addr:\n",
    "        pathlib.Path(save_addr).parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.ioff()\n",
    "\n",
    "    if not norm:\n",
    "        norm = make_color_norm(plot_data, method=norm_method)\n",
    "\n",
    "    # 2d color plot using scatter\n",
    "    # made changes here on the fig size\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(x=X[:, 0], y=X[:, 1],\n",
    "                s=3,\n",
    "                c=plot_data, cmap=cmap, norm=norm)\n",
    "    cbar = plt.colorbar()\n",
    "    \n",
    "\n",
    "    # plot monitors\n",
    "    if isinstance(X_monitor, np.ndarray):\n",
    "        plt.scatter(x=X_monitor[:, 0], y=X_monitor[:, 1],\n",
    "                    s=10, c='black')\n",
    "\n",
    "    # adjust plot window\n",
    "    plt.xlim((np.min(X[:, 0]), np.max(X[:, 0])))\n",
    "    plt.ylim((np.min(X[:, 1]), np.max(X[:, 1])))\n",
    "    \n",
    "    #overlap the state borders\n",
    "    for shape in state_sf:\n",
    "        x = [i[0] for i in shape.shape.points[:]]\n",
    "        y = [i[1] for i in shape.shape.points[:]]\n",
    "        plt.scatter(x,y,color='black',s=0.01)\n",
    "    \n",
    "    \n",
    "    if save_addr:\n",
    "        plt.savefig(save_addr, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        plt.ion()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_norm_pred = visual_util.make_color_norm(\n",
    "        ensemble_sample_val_mean[:,3],  # exclude \"resid\" vales from pal\n",
    "        method=\"percentile\")\n",
    "# prepare color norms for plt.scatter\n",
    "color_norm_unc = visual_util.make_color_norm(\n",
    "        ensemble_sample_val_var[:,3],  # use \"overall\" and \"mean\" for pal\n",
    "        method=\"percentile\")\n",
    "# prepare color norms for plt.scatter\n",
    "color_norm_weights = visual_util.make_color_norm(\n",
    "        ensemble_weights_val[:, 3:],  \n",
    "        method=\"percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    \n",
    "    _SAVE_ADDR_PREFIX_curr = 'CA_2010_2016/new_'+str(int(year))\n",
    "    X_train_curr = filter_year(X_train, year)\n",
    "    \n",
    "    #getting the re-ordered lat and lon\n",
    "    X_valid_reordered = filter_year(ensemble_sample_val_mean, year)[:, :3]\n",
    "#     X_valid_reordered = (X_valid_reordered - X_centr) / X_scale\n",
    "    X_valid_reordered_locations = X_valid_reordered[:, :2]\n",
    "    #(723716, 2)\n",
    "\n",
    "    \n",
    "    post_mean_dict = {\n",
    "        \"overall\": filter_year(ensemble_sample_val_mean, year)[:, 3],\n",
    "        \"mean\": filter_year(ensemble_mean_val_mean, year)[:, 3],\n",
    "        \"resid\": filter_year(mean_resid, year)[:, 3]\n",
    "    }\n",
    "\n",
    "    post_uncn_dict = {\n",
    "        \"overall\": filter_year(ensemble_sample_val_var, year)[:, 3],\n",
    "        \"mean\": filter_year(ensemble_mean_val_var, year)[:, 3],\n",
    "        \"resid\": filter_year(uncn_resid, year)[:, 3],\n",
    "        \"noise\": filter_year(uncn_noise, year)[:, 3]\n",
    "    }\n",
    "\n",
    "\n",
    "    weights_dict = {}\n",
    "    model_names = _MODEL_DICTIONARY['root']\n",
    "    \n",
    "    for i in range(len(model_names)): \n",
    "      weights_dict[model_names[i]] = filter_year(ensemble_weights_val, year)[:, i+3]\n",
    "\n",
    "    #print(color_norm_unc)\n",
    "\n",
    "    color_norm_ratio = visual_util.make_color_norm(\n",
    "        post_uncn_dict[\"noise\"] / post_uncn_dict[\"overall\"],\n",
    "        method=\"percentile\")\n",
    "    \n",
    "\n",
    "    \"\"\" 3.1. posterior predictive uncertainty \"\"\"\n",
    "    for unc_name, unc_value in post_uncn_dict.items():\n",
    "        save_name = os.path.join(_SAVE_ADDR_PREFIX_curr,\n",
    "                                 'ensemble_posterior_uncn_{}.png'.format(\n",
    "                                     unc_name))\n",
    "\n",
    "        color_norm = my_posterior_heatmap_2d(unc_value,\n",
    "                                                      X=X_valid_reordered_locations, X_monitor=X_train_curr,\n",
    "                                                      cmap='inferno_r',\n",
    "                                                      norm=color_norm_unc,\n",
    "                                                      norm_method=\"percentile\",\n",
    "                                                      save_addr=save_name)\n",
    "\n",
    "    \"\"\" 3.2. posterior predictive mean \"\"\"\n",
    "    for mean_name, mean_value in post_mean_dict.items():\n",
    "        save_name = os.path.join(_SAVE_ADDR_PREFIX_curr,\n",
    "                                 'ensemble_posterior_mean_{}.png'.format(\n",
    "                                    mean_name))\n",
    "        color_norm =my_posterior_heatmap_2d(mean_value,\n",
    "                                                      X=X_valid_reordered_locations, X_monitor=X_train_curr,\n",
    "                                                      cmap='RdYlGn_r',\n",
    "                                                      norm=color_norm_pred,\n",
    "                                                      norm_method=\"percentile\",\n",
    "                                                      save_addr=save_name)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" 3.3. model weights \"\"\"\n",
    "\n",
    "    for model_name, model_weight in weights_dict.items():\n",
    "        save_name = os.path.join(_SAVE_ADDR_PREFIX_curr,\n",
    "                                 'ensemble_weights_val_{}.png'.format(\n",
    "                                      model_name))\n",
    "\n",
    "        color_norm = my_posterior_heatmap_2d(model_weight,\n",
    "                                                      X=X_valid_reordered_locations, X_monitor=X_train_curr,\n",
    "                                                      cmap='viridis',\n",
    "                                                      norm=color_norm_weights,\n",
    "                                                      norm_method=\"percentile\",\n",
    "                                                      save_addr=save_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_bne_pred = pd.DataFrame(ensemble_sample_val_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5060384, 4)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_bne_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_bne_pred = overall_bne_pred.rename(columns={0:'lon', 1:'lat', 2:'time', 3:'bne_pred'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_bne_pred.to_csv('CA_2010_2016/CA_2010_2016_ensemble_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
